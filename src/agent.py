"""
ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¸ä»¤å¡”ï¼ˆRAGãƒ­ã‚¸ãƒƒã‚¯ï¼‰
è³ªå•å¿œç­”å‡¦ç†ã®ä¸­æ ¸ã¨ãªã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
"""

import time
from typing import List, Dict, Any, Optional
from datetime import datetime
from openai import OpenAI

from .configs import config
from .custom_logger import get_module_logger
from .models import (
    QuestionRequest, AnswerResponse, SearchResult, 
    ErrorResponse
)
from .prompts import prompts
from tool import get_unified_search_engine

logger = get_module_logger("agent")


class SupportAgent:
    """ã‚µãƒãƒ¼ãƒˆãƒœãƒƒãƒˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self):
        self.openai_client = None
        self.search_engine = None
        self._initialize()
    
    def _initialize(self):
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        try:
            logger.info("ã‚µãƒãƒ¼ãƒˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’åˆæœŸåŒ–ä¸­...")
            
            # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
            self.openai_client = OpenAI(api_key=config.OPENAI_API_KEY)
            
            # çµ±åˆæ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’å–å¾—
            self.search_engine = get_unified_search_engine()
            
            # å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
            self._health_check()
            
            logger.info("ã‚µãƒãƒ¼ãƒˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")
            
        except Exception as e:
            logger.error(f"ã‚µãƒãƒ¼ãƒˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            raise
    
    def _health_check(self):
        """ã‚·ã‚¹ãƒ†ãƒ ã®å¥å…¨æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            # OpenAIæ¥ç¶šãƒ†ã‚¹ãƒˆ
            self.openai_client.models.list()
            logger.info("OpenAIæ¥ç¶š: OK")
            
            # æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
            search_health = self.search_engine.health_check()
            if search_health['overall']:
                logger.info("æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³: OK")
            else:
                logger.warning("æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã®ä¸€éƒ¨ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            
        except Exception as e:
            logger.error(f"å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            raise
    
    async def process_question(
        self, 
        question_request: QuestionRequest
    ) -> AnswerResponse:
        """
        è³ªå•ã‚’å‡¦ç†ã—ã¦å›ç­”ã‚’ç”Ÿæˆ
        
        Args:
            question_request: è³ªå•ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        
        Returns:
            å›ç­”ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        """
        start_time = time.time()
        
        try:
            question = question_request.question
            context = question_request.context or {}
            
            logger.info(f"è³ªå•ã‚’å‡¦ç†ä¸­: '{question}'")
            
            # 1. æ¤œç´¢å®Ÿè¡Œ
            search_results, search_strategy = await self._search_knowledge_base(
                question, context
            )
            
            # 2. å›ç­”ç”Ÿæˆ
            if search_results:
                answer, confidence = await self._generate_answer_with_sources(
                    question, search_results, search_strategy
                )
            else:
                answer, confidence = await self._generate_no_results_answer(question)
                search_results = []
            
            # 3. ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä½œæˆ
            processing_time = time.time() - start_time
            
            response = AnswerResponse(
                answer=answer,
                confidence=confidence,
                sources=search_results,
                processing_time=processing_time
            )
            
            logger.info(f"è³ªå•å‡¦ç†å®Œäº†: {processing_time:.2f}ç§’, ä¿¡é ¼åº¦: {confidence:.2f}")
            return response
            
        except Exception as e:
            logger.error(f"è³ªå•å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            processing_time = time.time() - start_time
            
            # ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½œæˆ
            return AnswerResponse(
                answer="ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
                confidence=0.0,
                sources=[],
                processing_time=processing_time
            )
    
    async def _search_knowledge_base(
        self, 
        question: str, 
        context: Dict[str, Any]
    ) -> tuple[List[SearchResult], str]:
        """ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢"""
        try:
            # ã‚¹ãƒãƒ¼ãƒˆæ¤œç´¢ã‚’å®Ÿè¡Œ
            search_results, strategy = self.search_engine.smart_search(
                query=question,
                context=context
            )
            
            logger.info(f"æ¤œç´¢å®Œäº†: {len(search_results)}ä»¶ (æˆ¦ç•¥: {strategy})")
            
            # çµæœã‚’ãƒ­ã‚°ã«è¨˜éŒ²
            for i, result in enumerate(search_results[:3], 1):
                logger.debug(f"æ¤œç´¢çµæœ{i}: {result.source} (ã‚¹ã‚³ã‚¢: {result.score:.3f})")
            
            return search_results, strategy
            
        except Exception as e:
            logger.error(f"ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return [], "error"
    
    async def _generate_answer_with_sources(
        self, 
        question: str, 
        search_results: List[SearchResult],
        search_strategy: str
    ) -> tuple[str, float]:
        """æ¤œç´¢çµæœã‚’åŸºã«å›ç­”ã‚’ç”Ÿæˆ"""
        try:
            # æ¤œç´¢çµæœã‚’ã‚½ãƒ¼ã‚¹åˆ¥ã«åˆ†é¡
            faq_results = [r for r in search_results if r.metadata.get('type') == 'faq']
            manual_results = [r for r in search_results if r.metadata.get('type') == 'manual']
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é¸æŠ
            if faq_results and manual_results:
                # è¤‡æ•°ã‚½ãƒ¼ã‚¹çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                prompt = prompts.generate_multi_source_prompt(
                    question=question,
                    faq_results=faq_results,
                    manual_results=manual_results
                )
            elif faq_results:
                # FAQå°‚ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                prompt = prompts.generate_faq_prompt(question, faq_results)
            elif manual_results:
                # ãƒãƒ‹ãƒ¥ã‚¢ãƒ«å°‚ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                prompt = prompts.generate_manual_prompt(question, manual_results)
            else:
                # çµæœãªã—ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                prompt = prompts.generate_no_results_prompt(question)
            
            # OpenAIã§å›ç­”ç”Ÿæˆ
            response = self.openai_client.chat.completions.create(
                model=config.OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": prompts.SYSTEM_ROLE},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=800
            )
            
            answer = response.choices[0].message.content.strip()
            
            # ä¿¡é ¼åº¦ã‚’è¨ˆç®—
            confidence = self._calculate_confidence(search_results, search_strategy)
            
            logger.info(f"å›ç­”ç”Ÿæˆå®Œäº†: {len(answer)}æ–‡å­—, ä¿¡é ¼åº¦: {confidence:.2f}")
            return answer, confidence
            
        except Exception as e:
            logger.error(f"å›ç­”ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return "å›ç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚", 0.0
    
    async def _generate_no_results_answer(self, question: str) -> tuple[str, float]:
        """æ¤œç´¢çµæœãŒãªã„å ´åˆã®å›ç­”ã‚’ç”Ÿæˆ"""
        try:
            prompt = prompts.generate_no_results_prompt(question)
            
            response = self.openai_client.chat.completions.create(
                model=config.OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": prompts.SYSTEM_ROLE},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=400
            )
            
            answer = response.choices[0].message.content.strip()
            
            logger.info("æ¤œç´¢çµæœãªã—å›ç­”ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
            return answer, 0.1  # ä½ã„ä¿¡é ¼åº¦
            
        except Exception as e:
            logger.error(f"æ¤œç´¢çµæœãªã—å›ç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return prompts.generate_no_results_prompt(question), 0.0
    
    def _calculate_confidence(
        self, 
        search_results: List[SearchResult],
        search_strategy: str
    ) -> float:
        """å›ç­”ã®ä¿¡é ¼åº¦ã‚’è¨ˆç®—"""
        if not search_results:
            return 0.0
        
        # åŸºæœ¬ã‚¹ã‚³ã‚¢ï¼ˆæœ€é«˜ã‚¹ã‚³ã‚¢ã®çµæœã‚’é‡è¦–ï¼‰
        max_score = max(result.score for result in search_results)
        avg_score = sum(result.score for result in search_results) / len(search_results)
        
        # çµæœæ•°ã«ã‚ˆã‚‹èª¿æ•´
        result_count_factor = min(len(search_results) / 3.0, 1.0)
        
        # æ¤œç´¢æˆ¦ç•¥ã«ã‚ˆã‚‹èª¿æ•´
        strategy_factor = {
            "faq_focus": 0.9,
            "manual_focus": 0.8,
            "balanced": 0.85,
            "error": 0.3
        }.get(search_strategy, 0.7)
        
        # ä¿¡é ¼åº¦ã‚’è¨ˆç®—
        confidence = (max_score * 0.6 + avg_score * 0.4) * result_count_factor * strategy_factor
        
        # 0.0-1.0ã®ç¯„å›²ã«æ­£è¦åŒ–
        return max(0.0, min(1.0, confidence))
    
    def get_system_status(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã‚’å–å¾—"""
        try:
            status = {
                "agent_status": "healthy",
                "timestamp": datetime.now().isoformat(),
                "openai_connection": False,
                "search_engine_status": {},
                "version": "1.0.0"
            }
            
            # OpenAIæ¥ç¶šãƒã‚§ãƒƒã‚¯
            try:
                self.openai_client.models.list()
                status["openai_connection"] = True
            except:
                status["openai_connection"] = False
                status["agent_status"] = "degraded"
            
            # æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
            try:
                status["search_engine_status"] = self.search_engine.health_check()
                if not status["search_engine_status"]["overall"]:
                    status["agent_status"] = "degraded"
            except:
                status["search_engine_status"] = {"overall": False}
                status["agent_status"] = "degraded"
            
            return status
            
        except Exception as e:
            logger.error(f"ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return {
                "agent_status": "error",
                "timestamp": datetime.now().isoformat(),
                "error": str(e)
            }
    
    async def process_batch_questions(
        self, 
        questions: List[str]
    ) -> List[AnswerResponse]:
        """è¤‡æ•°ã®è³ªå•ã‚’ãƒãƒƒãƒå‡¦ç†"""
        try:
            logger.info(f"ãƒãƒƒãƒå‡¦ç†ã‚’é–‹å§‹: {len(questions)}ä»¶ã®è³ªå•")
            
            responses = []
            for i, question in enumerate(questions, 1):
                logger.info(f"ãƒãƒƒãƒå‡¦ç†ä¸­ ({i}/{len(questions)}): {question}")
                
                request = QuestionRequest(question=question)
                response = await self.process_question(request)
                responses.append(response)
            
            logger.info(f"ãƒãƒƒãƒå‡¦ç†å®Œäº†: {len(responses)}ä»¶ã®å›ç­”ã‚’ç”Ÿæˆ")
            return responses
            
        except Exception as e:
            logger.error(f"ãƒãƒƒãƒå‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return []


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_support_agent = None

def get_support_agent() -> SupportAgent:
    """ã‚µãƒãƒ¼ãƒˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _support_agent
    if _support_agent is None:
        _support_agent = SupportAgent()
    return _support_agent


async def main():
    """ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import asyncio
    
    try:
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        agent = get_support_agent()
        
        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã‚’è¡¨ç¤º
        status = agent.get_system_status()
        print(f"ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹: {status['agent_status']}")
        
        # ãƒ†ã‚¹ãƒˆè³ªå•
        test_questions = [
            "å‹¤æ€ ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã«ãƒ­ã‚°ã‚¤ãƒ³ã§ãã¾ã›ã‚“",
            "æœ‰çµ¦ç”³è«‹ã¯ã©ã“ã‹ã‚‰è¡Œã„ã¾ã™ã‹ï¼Ÿ",
            "ä¼šè­°å®¤ã®äºˆç´„æ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„"
        ]
        
        print("\nğŸ¤– ã‚µãƒãƒ¼ãƒˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ†ã‚¹ãƒˆ:")
        print("=" * 50)
        
        for question in test_questions:
            print(f"\nè³ªå•: {question}")
            print("-" * 30)
            
            request = QuestionRequest(question=question)
            response = await agent.process_question(request)
            
            print(f"ä¿¡é ¼åº¦: {response.confidence:.2f}")
            print(f"å‡¦ç†æ™‚é–“: {response.processing_time:.2f}ç§’")
            print(f"å‚ç…§æ•°: {len(response.sources)}ä»¶")
            print(f"å›ç­”:\n{response.answer}")
        
        return 0
        
    except Exception as e:
        logger.error(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return 1


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())